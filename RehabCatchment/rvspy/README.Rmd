---
title: "Catchment areas for rehabilitation centres: R versus Python"
output:
  rmarkdown::html_vignette:
    self_contained: no

  md_document:
    variant: markdown_github
---

```{r opts, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = TRUE,
  message = TRUE,
  width = 120,
  comment = "#>",
  fig.retina = 2,
  fig.path = "README-",
  fig.width = 10
)
```

The R and python analyses differed in 2 primary ways:

1. The R routing used a weighted network, so cars were preferentially routed
   along the weighted version, yet resultant distances calculated from the
   unweighted version. The python routing used a non-weighted graph, with the
   network instead reduced to only those ways usable by cars.
2. The R analyses sampled actual postcode addresses with the `PSMA` package,
   while the python analyses simply sampled random points within each postcode.

The effects of these 2 differences are compared here (in R code). This code is
merely a stripped-down version of code from the main document.


```{r initial-load, message = FALSE}
library(tidyverse)
library(sf)
library(units)
library(tmaptools)
```

# Data pre-processing

Load and clean Census Data
```{r load-basic-data}
postcodeboundariesAUS <- 
    file.path(here::here(), "ABSData", "Boundaries/POA_2016_AUST.shp") %>%
    sf::read_sf ()

basicDemographicsVIC <- file.path(here::here(), "ABSData",
                                  "2016 Census GCP Postal Areas for VIC",
                                  "2016Census_G01_VIC_POA.csv") %>%
    readr::read_csv()
basicDemographicsVIC <- select(basicDemographicsVIC, POA_CODE_2016, starts_with("Age_"), -starts_with("Age_psns_"))
```

JoinCensusAndBoundaries 
```{r join-demo-postcodes}
basicDemographicsVIC <- right_join(postcodeboundariesAUS,
                                   basicDemographicsVIC, 
                                   by=c("POA_CODE" = "POA_CODE_2016"))
```

Geocode and transform RehabNetwork 
```{r rehab-addresses}
rehab_addresses <- c(DandenongHospital = "Dandenong Hospital, Dandenong VIC 3175, Australia",
                     CaseyHospital = "62-70 Kangan Dr, Berwick VIC 3806, Australia",
                     KingstonHospital = "The Kingston Centre, Heatherton VIC 3202, Australia")
RehabLocations <- tmaptools::geocode_OSM(rehab_addresses, as.sf=TRUE)
RehabLocations <- sf::st_transform(RehabLocations,
                                   sf::st_crs(basicDemographicsVIC))
```

Postcodes surrounding rehab locations

```{r postcode-dists}
dist_to_loc <- function (geometry, location){
    units::set_units(st_distance(geometry, location)[,1], km)
}
dist_range <- units::set_units(10, km)

basicDemographicsVIC_old <- basicDemographicsVIC
basicDemographicsVIC <- mutate(basicDemographicsVIC,
       DirectDistanceToDandenong = dist_to_loc(geometry,RehabLocations["DandenongHospital", ]),
       DirectDistanceToCasey     = dist_to_loc(geometry,RehabLocations["CaseyHospital", ]),
       DirectDistanceToKingston  = dist_to_loc(geometry,RehabLocations["KingstonHospital", ]),
       DirectDistanceToNearest   = pmin(DirectDistanceToDandenong,
                                        DirectDistanceToCasey,
                                        DirectDistanceToKingston)
)
basicDemographicsRehab <- filter(basicDemographicsVIC,
                                 DirectDistanceToNearest < dist_range) %>%
        mutate(Postcode = as.numeric(POA_CODE16)) %>%
        select(-starts_with("POA_"))
```

## Data sampling

The major difference between the R and python code is the sampling method. The R
code sampled actual random addresses from postcodes, whereas the python code -
simply because of the unavailability of the amazing `PSMA` R package - could not
do this, and so sample random points from within the postcode polygons. These
two approaches are replicated here in R code, the first referred to as
`randomaddresses`, the second as `randomPoints`.

The `addressesPerPostcode` value below is modified by the estimated stroke rate
per postcode calculated in the python code.

```{r addressesPerPostcode}
addressesPerPostcode <- 1000
```
The python code has fewer postcodes than the R code, with numbers determined
manually here by comparing the corresponding maps. The reduced version
equivalent to the python code is:
```{r}
#mapview::mapview (basicDemographicsRehab_py)
removes <- c (40, 56, 57, 53, 43, 10, 7, 8, 29, 11, 1, 3)
index <- seq (nrow (basicDemographicsRehab))
basicDemographicsRehab_py <- basicDemographicsRehab [!index %in% removes, ]
```

Random addresses:
```{r random-addresses}
library(PSMA)
samplePCode <- function(pcode, number) {
  d <- fetch_postcodes(pcode)
  return(d[, .SD[sample(.N, min(number, .N))], by=.(POSTCODE)])
}

randomaddresses <- map(basicDemographicsRehab$Postcode,
                       samplePCode,
                       number=addressesPerPostcode) %>%
            bind_rows() %>%
            sf::st_as_sf(coords = c("LONGITUDE", "LATITUDE"),
                         crs=st_crs(basicDemographicsRehab),
                         agr = "constant")
randomaddresses_py <- map(basicDemographicsRehab_py$Postcode,
                       samplePCode,
                       number=addressesPerPostcode) %>%
            bind_rows() %>%
            sf::st_as_sf(coords = c("LONGITUDE", "LATITUDE"),
                         crs=st_crs(basicDemographicsRehab),
                         agr = "constant")
```

Random points:
```{r random-points}
randomPoints <- apply (basicDemographicsRehab, 1, function (i) {
                           x <- st_sample (i$geometry,
                                           size = addressesPerPostcode)
                           st_sf (POSTCODE = i$Postcode,
                                  geometry = x)
                         })
randomPoints <- do.call (rbind, randomPoints)
st_crs (randomPoints) <- 4326
randomPoints_py <- apply (basicDemographicsRehab_py, 1, function (i) {
                           x <- st_sample (i$geometry,
                                           size = addressesPerPostcode)
                           st_sf (POSTCODE = i$Postcode,
                                  geometry = x)
                         })
randomPoints_py <- do.call (rbind, randomPoints_py)
st_crs (randomPoints_py) <- 4326
```


Code to examine the distributions. The two are not shown here, to avoid junking
up the repo with unnecessary files, but there really is a striking difference -
the postcodes are much more concentrated where people actually live, and so much
greater overall spatial heterogeneity, while the random points have relatively
many more points in less populated areas.
```{r mapdeck, eval = FALSE}
library (mapdeck)
set_token(Sys.getenv("MAPBOX_TOKEN"))
mapdeck(location = c(145.2, -38), zoom = 14) %>%
    add_scatterplot (randomaddresses, radius = 2)
mapdeck(location = c(145.2, -38), zoom = 14) %>%
    add_scatterplot (randomPoints, radius = 2)
```


## Street Network

```{r postcode-bounding-polygon, eval = TRUE}
bounding_polygon <- sf::st_transform(basicDemographicsRehab,
                                     sf::st_crs(4326)) %>%
    sf::st_union () %>%
    sf::st_coordinates ()
bounding_polygon <- bounding_polygon [, 1:2]
```
```{r get-streetnet, eval = FALSE}
library(dodgr)
system.time (
dandenong_streets <- dodgr_streetnet (bounding_polygon, expand = 0, quiet = FALSE)
)
saveRDS (dandenong_streets, file = "../dandenong-streets.Rds")
```
```{r reload-streetnet-demo, eval = TRUE, message = FALSE}
dandenong_streets <- readRDS ("../dandenong-streets.Rds")
library (dodgr)
net <- weight_streetnet (dandenong_streets, wt_profile = "motorcar")
net <- net [which (net$d_weighted < .Machine$double.xmax), ]
```
An unweighted network analogous to that used in the python analyses can then be
created simply by
```{r unweight-net}
net_unwt <- net
net_unwt$d_weighted <- net_unwt$d
```
A final bit of pre-processing to speed up the following code:
```{r dodgr_vertices}
nodes <- dodgr_vertices (net) # same for both net and net_unwt
```

## direct sample of street network points within postcode boundary

Following the python code, simply sample a fixed number of random
points from the street network within the entire postcode boundary, as well as
simply from within the boundary itself.
```{r trulyRandomPoints}
npts <- 10000
pts_in_net <- as.matrix (nodes [sample (nrow (nodes), size = npts),
                         c ("x", "y")]) %>%
    as.data.frame () %>%
    st_as_sf (coords = c (1, 2)) %>%
    st_sf (crs = st_crs (basicDemographicsRehab))

assign_postcodes <- function (pts, basicDemographicsRehab)
{
    pts_in_postcodes <- st_contains (basicDemographicsRehab, pts)
    postcodes <- rep (NA, length (pts))
    for (i in seq (pts_in_postcodes))
        postcodes [pts_in_postcodes [[i]] ] <- basicDemographicsRehab$Postcode [i]
    st_sf (POSTCODE = postcodes,
           geometry = pts$geometry)
}

pts_in_net <- assign_postcodes (pts_in_net, basicDemographicsRehab)

# Then points randomly sample from within the bounding polygon of all postcodes
bp <- st_union (basicDemographicsRehab)
pts_in_poly <- st_sf (geometry = st_sample (bp, size = npts))
pts_in_poly <- assign_postcodes (pts_in_poly, basicDemographicsRehab)
```


That suffices to now examine the differences in estimated cases per centre.

## CasesPerCentre 

```{r cases-per-centre}
cases_per_centre <- function (randomxy, net, nodes, RehabLocations, stroke_rate)
{
    fromCoords <- st_coordinates (st_transform (randomxy, crs = 4326))
    fromIDX <- match_pts_to_graph (nodes, fromCoords, connected = TRUE)
    from <- nodes$id [fromIDX]
    toCoords <- st_coordinates (st_transform (RehabLocations, crs = 4326))
    to <- nodes$id [match_pts_to_graph (nodes, toCoords, connected = TRUE)]
    d <- dodgr_dists (net, from = from, to = to)

    DestNames <- c(rownames(RehabLocations), "Disconnected")
    DestNumber <- as.numeric (apply(d, MARGIN=1, which.min))
    DestNumber [is.na (DestNumber)] <- 4 # the disconnected points
    BestDestination <- DestNames[DestNumber]
    postcodes <- data.frame (POSTCODE = randomxy$POSTCODE,
                             DestNumber = DestNumber,
                             Destination = BestDestination,
                             stringsAsFactors = FALSE) %>%
        group_by (POSTCODE, DestNumber, Destination) %>%
        summarise (n = length (DestNumber))
    index <- match (postcodes$POSTCODE, stroke_rate$POSTCODE)
    postcodes$load <- stroke_rate$strokes [index]

    postcodes %>%
        filter (Destination != "Disconnected") %>%
        group_by (Destination) %>%
        summarise (total = sum (load)) %>%
        mutate (percent = 100 * total / sum (total))
}
```
Then run that function for the eight possible combinations of differences, first
loading the stroke rate estimates from the python code to use to load the final
postcode-based estimates.
```{r, cases-per-centre-output}
stroke_rate <- read.csv ("../../python/notebooks/data/postcode_strokes.csv",
                         stringsAsFactors = FALSE)
stroke_rate$POSTCODE <- substr (stroke_rate$POA_CODE, 4, 7)
library (knitr) # just for neat table output
kable (cases_per_centre (randomaddresses, net, nodes, RehabLocations, stroke_rate))
kable (cases_per_centre (randomaddresses, net_unwt, nodes, RehabLocations, stroke_rate))
kable (cases_per_centre (randomPoints, net, nodes, RehabLocations, stroke_rate))
kable (cases_per_centre (randomPoints, net_unwt, nodes, RehabLocations, stroke_rate))

# The `_py` addresses from the reduced set of postcodes
kable (cases_per_centre (randomaddresses_py, net, nodes, RehabLocations, stroke_rate))
kable (cases_per_centre (randomaddresses_py, net_unwt, nodes, RehabLocations, stroke_rate))
kable (cases_per_centre (randomPoints_py, net, nodes, RehabLocations, stroke_rate))
kable (cases_per_centre (randomPoints_py, net_unwt, nodes, RehabLocations, stroke_rate))

# And finally the "trulyRandomAddresses" simply sample from within the enclosing
# polygon of all postcodes
kable (cases_per_centre (pts_in_net, net, nodes, RehabLocations, stroke_rate))
kable (cases_per_centre (pts_in_net, net_unwt, nodes, RehabLocations, stroke_rate))
kable (cases_per_centre (pts_in_poly, net, nodes, RehabLocations, stroke_rate))
kable (cases_per_centre (pts_in_poly, net_unwt, nodes, RehabLocations, stroke_rate))
```

And that only makes a very small difference, in spite of the huge apparent
difference in distributions of random points, and still does not reproduce the
values generated in the python code.
